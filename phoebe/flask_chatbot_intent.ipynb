{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flask_chatbot_intent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fi9F10v4aCN"
      },
      "source": [
        "\n",
        "#!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ey2UgH54Ccs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fef07ab-c8c5-4cc3-ec1e-81186dbc63ab"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') #for NER\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.models import load_model  #USE JOBLIB instead\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from lxml import html\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "import calendar"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mVugFst4hog"
      },
      "source": [
        "\n",
        "\n",
        "#load pickled model\n",
        "model = load_model(\"drive/My Drive/phoebe/chatbot_model.h5\")\n",
        "#load files\n",
        "intents = json.loads(open('drive/My Drive/phoebe/intents.json').read())\n",
        "words = pickle.load(open('drive/My Drive/phoebe/words.pkl','rb'))\n",
        "classes = pickle.load(open('drive/My Drive/phoebe/classes.pkl','rb'))\n",
        "\n",
        "#define our global variable\n",
        "global context\n",
        "context = {'userID': 'none'}\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghTzSH5XbW8r"
      },
      "source": [
        "#function\n",
        "def bow(sentence, words, show_details=True): #show_details = false for deployment\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    #print(return_list)\n",
        "    return return_list\n",
        "\n",
        "#google weather function\n",
        "def search_weather(location):\n",
        "  answer = \"The weather in \"+location+\" is \"\n",
        "  query = \"Weather in \"+location+\" today\"\n",
        "  try:\n",
        "    article = ''\n",
        "    search_result_list = list(search(query, lang='en', num=10, pause=1))\n",
        "    page = requests.get(search_result_list[0]) #scraping weather.com\n",
        "    \n",
        "    if page.status_code == 200:\n",
        "      soup = BeautifulSoup(page.text, features='lxml')\n",
        "      \n",
        "      #all_articles = soup.find('div', {\"class\" : \"_-_-components-src-organism-CurrentConditions-CurrentConditions--primary--2DOqs\" })\n",
        "      all_articles = soup.find('span', {\"class\": \"CurrentConditions--tempValue--3KcTQ\"})\n",
        "      if all_articles != None:\n",
        "        result = all_articles.text  #only temperature\n",
        "        ar = soup.find('div', {'data-testid': \"precipPhrase\"}).text  #details\n",
        "        \n",
        "        return answer+result+'F with '+str(ar)\n",
        "      else: \n",
        "        raise Exception(\"Sorry, facing an error\")\n",
        "  \n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    result = \"I can not find an answer for that. Try a different query!\"\n",
        "    return result\n",
        "\n",
        "#function to find name-entity in user query\n",
        "def ner_find(query): #fins POS of query words\n",
        "    tok = nltk.word_tokenize(query)\n",
        "    res = nltk.pos_tag(tok)\n",
        "    #chunking-> adding structure above and below the tagged sentences\n",
        "    res_chunk = nltk.ne_chunk(res)\n",
        "    result = []\n",
        "    for x in str(res_chunk).split('\\n'):\n",
        "      if '/NNP' in x:  \n",
        "          names = re.findall(r'([a-zA-Z]+/NN[a-zA-Z])', x)\n",
        "          for l in names:\n",
        "            result.extend(l[0:-4])\n",
        "            result.extend(' ')\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "#google Celebrity AGE on wiki function\n",
        "def search_wiki(query):\n",
        "  #NER for celebrity name\n",
        "  name = ner_find(query)\n",
        "  #query = \"How old is \"+name\n",
        "  answer = name+'is '\n",
        "  try:\n",
        "    article = ''\n",
        "    search_result_list = list(search(query, lang='en', num=10, pause=1))\n",
        "    page = requests.get(search_result_list[0]) #scraping wiki on the right card\n",
        "    \n",
        "    if page.status_code == 200: #all okay that is\n",
        "      soup = BeautifulSoup(page.text, features='lxml')\n",
        "      \n",
        "      all_articles = soup.find('span', {\"class\": \"noprint ForceAgeToShow\"})\n",
        "      if all_articles != None:\n",
        "        result = all_articles.text  #only age number\n",
        "        ar = soup.find('span', {'class': \"bday\"}).text  #birthday in date\n",
        "        #convert birthdate to month, year format\n",
        "        date = calendar.month_name[int(ar.split('-')[1])]\n",
        "        \n",
        "        return answer+str(re.findall(r'[0-9]+', result)[0])+' years old. Born on '+str(ar.split('-')[2])+' '+str(date)+\" \"+str(ar.split('-')[0])\n",
        "      else: \n",
        "        raise Exception(\"Sorry, facing an error\")\n",
        "  \n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    result = \"I can not find an answer for that. Try a different query!\"\n",
        "    return result\n",
        "\n",
        "\n",
        "#function that calls other google search functions\n",
        "google_search_func = {'Search weather': search_weather, 'search celebrity': search_wiki} #argument is passed when called\n",
        "\n",
        "#driver function version #2\n",
        "def chatbot_response_2(msg, userID='userID', show_details = False):\n",
        "    results = predict_class(msg, model)\n",
        "\n",
        "    if context['userID'] is 'none':\n",
        "      if results:\n",
        "          # loop as long as there are matches to process\n",
        "          while results:\n",
        "            for i in intents['intents']:\n",
        "                  #print(\"chatbot_response i : \", i)\n",
        "                  # find a tag matching the first result\n",
        "                  if i['tag'] == results[0]['intent']:\n",
        "                      # set context for this intent if necessary\n",
        "                      if 'context' in i:\n",
        "                          if show_details: print ('context:', i['context'])\n",
        "                          context['userID'] = i['context']\n",
        "\n",
        "                      # check if this intent is contextual and applies to this user's conversation\n",
        "                      \n",
        "                      if not 'context_filter' in i or (userID in context and 'context_filter' in i and i['context_filter'] == context['userID']):\n",
        "                          if show_details: print ('tag:', i['tag'])\n",
        "                          # a random response from the intent\n",
        "                          return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)\n",
        "    else:\n",
        "      for i in intents['intents']:\n",
        "        if context['userID'] in i['tag']:\n",
        "           print(i['responses'][0])\n",
        "           msg = input('Enter Location: ') #give proper prompt on weather/location query\n",
        "           return google_search_func[i['tag']](msg)  #call based on tag\n",
        "           #have to respond and then call google search function acc to context\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCgVzgNjVBIf",
        "outputId": "1d83e15e-3c7f-4cdf-c582-1706b043301d"
      },
      "source": [
        "#TEST--------------------------------------------------------------------------------------------\n",
        "user_query = \"What is the weather?\"\n",
        "print(chatbot_response_2(user_query))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking...\n",
            "Enter here: Bangalore\n",
            "The weather in Bangalore is 71Â°F with 15% chance of rain through 8 pm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jXWlIPxTCdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f2d1e21e-8c8b-4ab9-e0e1-2a6691f23d66"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HTTP Error 429: Too Many Requests\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I can not find an answer for that. Try a different query!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    }
  ]
}